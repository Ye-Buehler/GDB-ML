{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Import classes and set the src path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pandarallel import pandarallel\n",
    "import shutil\n",
    "\n",
    "# Add the path to 'src' (where 'gdb_ml' is located)\n",
    "src_path = os.path.abspath(\".../gdb_ml/src\")  \n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from gdb_ml import ChemUtils\n",
    "# Create an instance\n",
    "chem_utils = ChemUtils()\n",
    "\n",
    "from gdb_ml import PropertiesCalculator\n",
    "# Create an instance\n",
    "properties_calculator = PropertiesCalculator()\n",
    "\n",
    "from gdb_ml import DataProcessor\n",
    "# Create an instance\n",
    "data_processor = DataProcessor()\n",
    "\n",
    "# Suppress RDKit warnings and errors\n",
    "RDLogger.DisableLog('rdApp.error')\n",
    "\n",
    "# Initialize pandarallel\n",
    "pandarallel.initialize(progress_bar=True)  # Enable progress bar for tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Canonicalize the SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...graphs_or_mols.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, has_header=0, add_header=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Canonicalized SMILES\"] = \"\"\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "    try:  \n",
    "        smiles = df['SMILES'][q]\n",
    "        random_one_smiles = chem_utils.canonicalize_smiles(smiles)\n",
    "        #print(token)\n",
    "        df.loc[q, 'Canonicalized SMILES'] = str(random_one_smiles)\n",
    "\n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "print(count, 'invalids')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE =  \"...graphs_or_mols_canonicalized.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df['Canonicalized SMILES'], FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Randomize the SMILES (alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...graphs_or_mols.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, SEPRATOR= \",\", has_header=0, add_header=1, COLUMN_NAME_INPUT= [\"SMILES\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"SMILES\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"SMILES\"], keep='last').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Randomized SMILES\"] = \"\"\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "    try:  \n",
    "        smiles = df['SMILES'][q]\n",
    "        random_one_smiles = chem_utils.smiles_randomization(smiles)\n",
    "        #print(token)\n",
    "        df.loc[q, 'Randomized SMILES'] = str(random_one_smiles)\n",
    "\n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "print(count, 'invalids')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.FDV filter (MC1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FDV\"] = \"\"\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "    try:  \n",
    "        smiles = df['Canonicalized SMILES'][q]\n",
    "        random_one_smiles = properties_calculator.divalent_nodes_fraction(smiles)\n",
    "        #print(token)\n",
    "        df.loc[q, 'FDV'] = str(random_one_smiles)\n",
    "\n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "print(count, 'invalids')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'FDV' to numeric, setting errors='coerce' to handle non-numeric values\n",
    "df['FDV'] = pd.to_numeric(df['FDV'], errors='coerce')\n",
    "\n",
    "# Filter the rows where 'FDV' > 0.4\n",
    "df = df[df['FDV'] > 0.4].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE = \"...fdv_filtered.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df['Canonicalized SMILES'], FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Graph extraction (from SMILES to character-based graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...mols.smi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, has_header=0, add_header=1, SEPRATOR = \",\" ,COLUMN_NAME_INPUT= [\"SMILES\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Character-based Conversion']= \"\"\n",
    "\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "    \n",
    "    try:  \n",
    "        smiles = df['SMILES'][q]\n",
    "        df.loc[q, 'Character-based Conversion'] = chem_utils.graph_convert(smiles)\n",
    "        \n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "\n",
    "print(count, 'invalids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE = \"...extracted_graphs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df, FILE_PATH_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df['Character-based Conversion'], FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Concatenate the SMILES with dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...canonicalized.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, SEPRATOR= \"\\t\", has_header=0, add_header=1, COLUMN_NAME_INPUT= [\"SMILES\",\"Length\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "n = len(df)\n",
    "\n",
    "# Compute split indices\n",
    "split = int(n * 0.33)  # First 33% and last 33%\n",
    "middle_start = split\n",
    "middle_end = n - split  # This ensures 34% of middle remains untouched\n",
    "\n",
    "# Create a new column for concatenated SMILES (default: keep as single molecules)\n",
    "df[\"Concatenated SMILES\"] = df[\"SMILES\"]\n",
    "\n",
    "# Concatenate the first 33% with the last 33%\n",
    "for i in range(split):\n",
    "    shortest_smiles = df.loc[i, \"SMILES\"]\n",
    "    longest_smiles = df.loc[n - i - 1, \"SMILES\"]\n",
    "    df.loc[i, \"Concatenated SMILES\"] = f\"{shortest_smiles}.{longest_smiles}\"\n",
    "\n",
    "# Remove the last 33% (since they have been concatenated)\n",
    "df = df.iloc[:middle_end].reset_index(drop=True)  # Keep the first 33% (modified) + middle 34% (unchanged)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE =  \"...concatenated.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df['Concatenated SMILES'], FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Tokenize the SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...concatenated.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, has_header=0, add_header=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Token\"] = \"\"\n",
    "\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "\n",
    "    try:  \n",
    "        smiles = df['SMILES'][q]\n",
    "        token = data_processor.tokenize_smiles(smiles)\n",
    "        #print(token)\n",
    "        df.loc[q, 'Token'] = str(token)\n",
    "\n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "\n",
    "print(count, 'invalids')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE = \"...tokenized.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df['Token'], FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for both keys and values files of train or validation set\n",
    "file1 = \"...keys_tokenized.txt\"\n",
    "file2 = \"...values_tokenized.txt\"\n",
    "\n",
    "# Read both files\n",
    "with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "    lines1 = f1.readlines()\n",
    "    lines2 = f2.readlines()\n",
    "\n",
    "# Ensure both files have the same number of lines\n",
    "if len(lines1) != len(lines2):\n",
    "    raise ValueError(\"Files have different numbers of lines!\")\n",
    "\n",
    "# Combine lines and shuffle\n",
    "combined = list(zip(lines1, lines2))\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split back into separate lists\n",
    "shuffled_lines1, shuffled_lines2 = zip(*combined)\n",
    "\n",
    "# Write the shuffled lines back to the files\n",
    "with open(file1, 'w', encoding='utf-8') as f1, open(file2, 'w', encoding='utf-8') as f2:\n",
    "    f1.writelines(shuffled_lines1)\n",
    "    f2.writelines(shuffled_lines2)\n",
    "\n",
    "print(\"Files shuffled successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Detokenize the generated mols from transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...generated_mols.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAME_OUTPUT = [\"Generated SMILES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_file_with_badlines(FILE_PATH_READ, COLUMN_NAME_OUTPUT)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Detokenized']=\"\"\n",
    "\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "\n",
    "    try:  \n",
    "        tokens_string = df['Generated SMILES'][q]\n",
    "        detokenized_string = data_processor.detokenize_smiles(tokens_string)\n",
    "        df.loc[q,'Detokenized'] = detokenized_string \n",
    "\n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "\n",
    "print(count, 'invalids')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE = \"...detokenized.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df['Detokenized'], FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Evaluations of the generated mols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Apend the log probs and calculate the validity, with canonicalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. For single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...generated_mols.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, COLUMN_NAME_INPUT=['SMILES'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"..._log_probs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs = data_processor.load_data(FILE_PATH_READ, COLUMN_NAME_INPUT=['log prob'])\n",
    "df_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SMILES': df['SMILES'], 'Log Probs': df_cs['log prob']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validity, df_valid = properties_calculator.validity(df)\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE = \"...valid_canonicalized_with_log_probs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df_valid, FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. For multiple files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### i. Irregular file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "folder_path = \".../generated_mols\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "\n",
    "# Sort the list of files\n",
    "file_list.sort()\n",
    "\n",
    "print(len(file_list), file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process files in the specified range, based on {suffix}\n",
    "\n",
    "# Define a function to process a single file\n",
    "def process_file(file):\n",
    "    # Define file paths\n",
    "    FILE_PATH_READ_1 = f\"...generated_mols/{file}\"\n",
    "    FILE_PATH_READ_2 = f\"...log_prob/{file}_log_probs\"\n",
    "    \n",
    "    file2 = file.split(\".txt\")[0]\n",
    "    FILE_PATH_SAVE = f\"...valid_canonicalized_with_log_probs/{file2}_valid_canonicalized_with_log_probs.txt\"\n",
    "\n",
    "    # Process the file and return the results\n",
    "    validity, df_valid = data_processor.detokenize_append_log_prob(FILE_PATH_READ_1, FILE_PATH_READ_2, FILE_PATH_SAVE)\n",
    "    return validity, df_valid\n",
    "\n",
    "# Convert file list to a pandas Series\n",
    "file_series = pd.Series(file_list)\n",
    "\n",
    "# Use pandarallel to apply the process_file function to each file\n",
    "results = file_series.parallel_apply(process_file)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "print(f\"Runtime: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ii. File names with suffix (alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Generate file range automatically\n",
    "file_range = [''.join(pair) for pair in product('abcdefghijklmnopqrstuvwxyz', repeat=2)]\n",
    "\n",
    "# Slice the list for a specific range (e.g., 'aa' to 'a')\n",
    "start = file_range.index('at')\n",
    "end = file_range.index('aw') + 1\n",
    "file_range = file_range[start:end]\n",
    "\n",
    "print(file_range, len(file_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress RDKit warnings and errors\n",
    "RDLogger.DisableLog('rdApp.error')\n",
    "\n",
    "# Process files in the specified range, based on {suffix}\n",
    "for suffix in file_range:\n",
    "    FILE_PATH_READ_1 = f\"generated_mols_{suffix}.txt\"\n",
    "\n",
    "    FILE_PATH_READ_2 = f\"...{suffix}.txt_log_probs\"\n",
    "\n",
    "    FILE_PATH_SAVE = f\"..._{suffix}_valid_canonicalized_with_log_probs.txt\"\n",
    "    \n",
    "    validity, df_valid = data_processor.detokenize_append_log_prob(FILE_PATH_READ_1, FILE_PATH_READ_2, FILE_PATH_SAVE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### iii.Merge all the valid dfs appended with log porbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = \"/...valid_canonicalized_with_log_probs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAME_INPUT = [\"SMILES\", \"Log Probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_log_prob = data_processor.append_dfs_in_folder(FOLDER_PATH, COLUMN_NAME_INPUT)\n",
    "df_valid_log_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged files:\n",
    "FILE_PATH_SAVE = \"...valid_canonicalized_with_log_prob.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df_valid_log_prob , FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Calculate the uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...valid_canonicalized_with_log_prob.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, has_header=0, add_header=1, COLUMN_NAME_INPUT= [\"SMILES\", \"Log Probs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness, df_unique = properties_calculator.uniqueness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged files:\n",
    "FILE_PATH_SAVE = \"...valid_unique.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df_unique, FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Claculate the novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...valid_unique.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = data_processor.load_data(FILE_PATH_READ, has_header=0, add_header=1, COLUMN_NAME_INPUT= [\"SMILES\", \"Log Probs\"])\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...train_or_validation_values.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_processor.load_data(FILE_PATH_READ)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty, df_novel = properties_calculator.novelty(df_unique, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Properties profiling of the generated mols (QED, SAS, Fsp3, C-atoms Fraction, logP, NPscore, weight...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For single file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...valid_unique.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processor.load_data(FILE_PATH_READ, has_header=0, add_header=1,COLUMN_NAME_INPUT=[\"SMILES\", \"Log Prob\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_calculator.weight(\"c1ccccc1N\")\n",
    "properties_calculator.npscore(\"c1ccccc1N\")\n",
    "properties_calculator.logP(\"c1ccccc1N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['QED']= \"\"\n",
    "df['SAscore']= \"\"\n",
    "df['Fsp3']= \"\"\n",
    "df['C-atoms Fraction']= \"\"\n",
    "\n",
    "count = 0\n",
    "my_list_invaild = []\n",
    "\n",
    "for q in tqdm(range(0, len(df)), desc = 'Loop 1'):\n",
    "    \n",
    "    try:  \n",
    "        smiles = df['SMILES'][q]\n",
    "        df.loc[q, 'SAscore'] = properties_calculator.sascore(smiles)\n",
    "        df.loc[q, 'QED'] = properties_calculator.qed(smiles)\n",
    "        df.loc[q, 'Fsp3'] = properties_calculator.fsp3(smiles)\n",
    "        df.loc[q, 'C-atoms Fraction'] = properties_calculator.fraction_c(smiles)\n",
    "        \n",
    "    except:\n",
    "        count += 1\n",
    "        my_list_invaild.append(q)\n",
    "        continue\n",
    "\n",
    "print(count, 'invalids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_SAVE = \"...properties.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.save_to_file(df, FILE_PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. For multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "folder_path = \".../valid_unique\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Sort the list of files\n",
    "file_list.sort()\n",
    "\n",
    "print(len(file_list), file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Process files in the specified range, based on {suffix}\n",
    "\n",
    "# Define the function to process a single file\n",
    "def process_file(file):\n",
    "    FILE_PATH_READ = f\".../with_log_prob/{file}\"\n",
    "    \n",
    "    file2 = file.split(\".txt\")[0]\n",
    "\n",
    "    FILE_PATH_SAVE_QED_SAS_FSP3_CF = f\".../{file2}_log_prob.txt\"\n",
    "\n",
    "    # Call the properties_calculator function\n",
    "    properties_calculator.multi_qed_sas_fsp3_cf(FILE_PATH_READ, FILE_PATH_SAVE_QED_SAS_FSP3_CF)\n",
    "\n",
    "\n",
    "# Convert the file list into a pandas Series\n",
    "file_series = pd.Series(file_list)\n",
    "\n",
    "# Use pandarallel to process files in parallel\n",
    "file_series.parallel_apply(process_file)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "print(f\"Runtime: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.Check the undesired FG or structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Filter1-10'] = df['SMILES'].apply(properties_calculator.undesired_FG_check)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed = df[\n",
    "    df['Filter1-10'].apply(\n",
    "        lambda x: isinstance(x, list) and len(x) > 0 and x[0] == False\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "df_failed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. For multiple files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "folder_path = \".../failed\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Sort the list of files\n",
    "file_list.sort()\n",
    "\n",
    "print(len(file_list), file_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Check the passed and failed molcules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process files in the specified range, based on {suffix}\n",
    "\n",
    "# Define the function to process a single file\n",
    "def process_file(file):\n",
    "    FILE_PATH_READ = f\".../failed/{file}\"\n",
    "    \n",
    "    file2 = file.split(\".txt\")[0]\n",
    "\n",
    "    FILE_PATH_SAVE_PASS = f\".../{file2}_desired_mols.txt\"\n",
    "    \n",
    "    FILE_PATH_SAVE_FAILED = f\".../{file2}_undesired_mols.txt\"\n",
    "    \n",
    "    # Call the properties_calculator function\n",
    "    properties_calculator.undesired_FG_details(FILE_PATH_READ, FILE_PATH_SAVE_PASS, FILE_PATH_SAVE_FAILED)\n",
    "\n",
    "\n",
    "# Convert the file list into a pandas Series\n",
    "file_series = pd.Series(file_list)\n",
    "\n",
    "# Use pandarallel to process files in parallel\n",
    "file_series.parallel_apply(process_file)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "print(f\"Runtime: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Show the details of the failed molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_READ = \"...undesired_mols.txt\"\n",
    "\n",
    "properties_calculator.show_undesired_FG_details(FILE_PATH_READ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('rxnmapper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a707b3da20c1aa30295684aa0dc8a4698d4dc111ee08bc3c29d98e944d9b781c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
